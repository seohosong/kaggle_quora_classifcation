{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gensim\nimport regex\nfrom tqdm import tqdm_notebook, trange, tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils import data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.utils.data\nimport torchvision\n\n\nclass ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n    \"\"\"Samples elements randomly from a given list of indices for imbalanced dataset\n    Arguments:\n        indices (list, optional): a list of indices\n        num_samples (int, optional): number of samples to draw\n    \"\"\"\n\n    def __init__(self, dataset, indices=None, num_samples=None):\n                \n        # if indices is not provided, \n        # all elements in the dataset will be considered\n        self.indices = list(range(len(dataset))) \\\n            if indices is None else indices\n            \n        # if num_samples is not provided, \n        # draw `len(indices)` samples in each iteration\n        self.num_samples = len(self.indices) \\\n            if num_samples is None else num_samples\n            \n        # distribution of classes in the dataset \n        label_to_count = {}\n        for idx in self.indices:\n            label = self._get_label(dataset, idx)\n            if label in label_to_count:\n                label_to_count[label] += 1\n            else:\n                label_to_count[label] = 1\n                \n        # weight for each sample\n        weights = [1.0 / label_to_count[self._get_label(dataset, idx)]\n                   for idx in self.indices]\n        self.weights = torch.DoubleTensor(weights)\n\n    def _get_label(self, dataset, idx):\n        dataset_type = type(dataset)\n        if dataset_type is torchvision.datasets.MNIST:\n            return dataset.train_labels[idx].item()\n        elif dataset_type is torchvision.datasets.ImageFolder:\n            return dataset.imgs[idx][1]\n        else:\n            return dataset.labels[idx]\n            #raise NotImplementedError\n                \n    def __iter__(self):\n        return (self.indices[i] for i in torch.multinomial(\n            self.weights, self.num_samples, replacement=True))\n\n    def __len__(self):\n        return self.num_samples","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_csv = pd.read_csv('/kaggle/input/quora-insincere-questions-classification/train.csv')\ntest_csv = pd.read_csv('/kaggle/input/quora-insincere-questions-classification/test.csv')\nraw_csv.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_csv['len_question'] = raw_csv['question_text'].apply(lambda x : len(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len_table = raw_csv[raw_csv['len_question'] < 120]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(len_table['len_question'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainset, valset = train_test_split(raw_csv, test_size = 0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model = gensim.models.KeyedVectors.load_word2vec_format('embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin', binary=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_questions = trainset['question_text'].tolist()\ntrain_labels = trainset['target'].tolist()\nval_questions = valset['question_text'].tolist()\nval_labels = valset['target'].tolist()\ntest_questions = test_csv['question_text'].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    \n    # Common\n    text = regex.sub(\"(?s)<ref>.+?</ref>\", \"\", text) # remove reference links\n    text = regex.sub(\"(?s)<[^>]+>\", \"\", text) # remove html tags\n    text = regex.sub(\"&[a-z]+;\", \"\", text) # remove html entities\n    text = regex.sub(\"(?s){{.+?}}\", \"\", text) # remove markup tags\n    text = regex.sub(\"(?s){.+?}\", \"\", text) # remove markup tags\n    text = regex.sub(\"(?s)\\[\\[([^]]+\\|)\", \"\", text) # remove link target strings\n    text = regex.sub(\"(?s)\\[\\[([^]]+\\:.+?]])\", \"\", text) # remove media links\n    \n    text = regex.sub(\"[']{5}\", \"\", text) # remove italic+bold symbols\n    text = regex.sub(\"[']{3}\", \"\", text) # remove bold symbols\n    text = regex.sub(\"[']{2}\", \"\", text) # remove italic symbols\n    \n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_train_questions = []\nfor question in train_questions:\n    new_train_questions.append(clean_text(question))\nnew_val_questions = []\nfor question in val_questions:\n    new_val_questions.append(clean_text(question))\nnew_test_questions = []\nfor question in test_questions:\n    new_test_questions.append(clean_text(question))\n\nall_sentences = new_train_questions + new_val_questions + new_test_questions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_dict(sen_list):\n    word2ix = {'<pad>': 0, '<UNK>': 1}\n    ix2word = {0: '<pad>', 1: '<UNK>'}\n    cnt = 0\n    \n    for sentence in sen_list:\n        for word in sentence.split():\n            if word in word2ix:\n                pass\n            else:\n                word2ix[word] = cnt\n                ix2word[cnt] = word\n                cnt += 1\n    \n    return word2ix, ix2word, cnt+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word2ix, ix2word, num_word = make_dict(all_sentences)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(num_word)\nword2ix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class QuestionData(data.Dataset):\n    \n    def __init__(self, questions, labels, word2ix, ix2word, num_word, max_length):\n        self.questions = questions\n        self.labels = labels\n        self.word2ix = word2ix\n        self.ix2word = ix2word\n        self.num_word = num_word\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, ix):\n        question = self.questions[ix]\n        q_ix = []\n        for word in question.split():\n            q_ix.append(self.word2ix[word])\n        if len(q_ix) < self.max_length:\n            q_ix += [0 for i in range(self.max_length - len(q_ix))]\n        else:\n            q_ix = q_ix[:self.max_length]\n            \n        return torch.tensor(q_ix, dtype=torch.long), torch.tensor([self.labels[ix]], dtype=torch.float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainset = QuestionData(new_train_questions, train_labels, word2ix, ix2word, num_word, 75)\nvalset = QuestionData(new_val_questions, val_labels, word2ix, ix2word, num_word, 75)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainloader = data.DataLoader(trainset, sampler=ImbalancedDatasetSampler(trainset), batch_size=16)\nvalloader = data.DataLoader(valset, batch_size=16, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BiLSTMClassifier(nn.Module):\n    \n    def __init__(self, num_word, embed_dim, hidden_dim, num_layer, dropout=0.2, pretrained=None, bidirectional=True):\n        super(BiLSTMClassifier, self).__init__()\n        self.embedding = nn.Embedding(num_word, embed_dim)\n        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layer, dropout=dropout, batch_first=True, bidirectional=bidirectional)\n        self.fc1 = nn.Linear(hidden_dim*2, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, 1)\n    \n    def forward(self, x):\n        x = self.embedding(x)\n        out, (hidden, cell) = self.lstm(x)\n        #dropout = F.dropout(out[:,-1,:])\n        dropout = out[:,-1,:]\n        logit = F.relu(self.fc1(dropout))\n        logit = self.fc2(logit)\n        return logit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class AttentionalClassifier(nn.Module):\n    \n    def __init__(self, num_word, embed_dim, hidden_dim, num_layer, dropout=0.2, pretrained=None, bidirectional=True):\n        super(AttentionalClassifier, self).__init__()\n        self.embedding = nn.Embedding(num_word, embed_dim)\n        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layer, dropout=dropout, batch_first=True, bidirectional=bidirectional)\n        self.fc1 = nn.Linear(hidden_dim*4, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, 1)\n    \n    def forward(self, x):\n        x = self.embedding(x)\n        out, (hidden, cell) = self.lstm(x) # 1 x 75 x 600\n        #out[:, -1, :] #1 x 600\n        out_T = out[:, -1, :].unsqueeze(1).transpose(1,2)\n        atten_weight = torch.bmm(out, out_T) #8 x 75 x 1\n        atten_applied = torch.bmm(atten_weight.transpose(1,2), out).squeeze(1)        \n        atten_combined = torch.cat([atten_applied, out[:, -1, :]], dim=1)\n        \n        dropout = F.dropout(atten_combined)\n        #dropout = F.dropout(out[:,-1,:])\n        logit = F.relu(self.fc1(dropout))\n        logit = self.fc2(logit)\n        return logit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CNNClassifier(nn.Module):\n    \n    def __init__(self, num_word, embed_dim, hidden_dim, pretrained=None):\n        super(CNNClassifier, self).__init__()\n        self.embedding = nn.Embedding(num_word, embed_dim)\n        \n        self.cnn_1 = nn.Conv1d(embed_dim, hidden_dim, kernel_size=3)\n        self.cnn_2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, stride=2)\n        \n        self.fc1 = nn.Linear(36, 30)\n        self.fc2 = nn.Linear(30, 1)\n    \n    def forward(self, x):\n        x = self.embedding(x)\n        x = x.transpose(1,2)\n        x = F.relu(self.cnn_1(x))\n        out = F.relu(self.cnn_2(x))\n        dropout = F.dropout(out[:,-1,:])\n        logit = F.relu(self.fc1(dropout))\n        logit = self.fc2(logit)\n        return logit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lstm_model = BiLSTMClassifier(num_word, 300, 64, 2).to(device)\nlstm_model = AttentionalClassifier(num_word, 300, 64, 2).to(device)\ncnn_model = CNNClassifier(num_word, 300, 64).to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(model, num_epoch=5, print_every=1000):\n    optimizer = optim.Adam(params=model.parameters(), lr=1e-2)\n    model.train()\n    \n    for epoch in tqdm(range(num_epoch)):\n    \n        training_loss = 0.0\n        for i, (data, label) in enumerate(tqdm_notebook(trainloader)):\n            data, label = data.to(device), label.to(device)\n            #label = label.unsqueeze(1)\n\n            #out = lstm_model(data)\n            out = model(data)\n            loss = F.binary_cross_entropy_with_logits(out, label)\n            training_loss += loss.item()\n            \n            loss.backward()\n            #nn.utils.clip_grad_norm(model.parameters(), 2)\n            optimizer.zero_grad()\n            optimizer.step()\n\n            if (i+1) % print_every == 0:\n                print('Step %d | loss %0.4f' %(i+1, training_loss/print_every))\n                training_loss = 0.0\n                \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test(model, loader):\n    model.eval()\n    with torch.no_grad():\n        total = 0\n        correct = 0\n        \n        for i, (data, label) in enumerate(loader):\n            data, label = data.to(device), label.to(device)\n            out = torch.sigmoid(model(data))\n            result = out.detach()#.squeeze(1)\n            result[result >= 0.5] = 1\n            result[result < 0.5] = 0\n            correct += (result == label).sum().item()\n            total += result.shape[0]\n\n    print('Total Accuracy : %0.4f' %(correct / total))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = train(lstm_model, 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test(lstm_model, valloader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}